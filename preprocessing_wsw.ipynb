{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_date_time</th>\n",
       "      <th>building_number</th>\n",
       "      <th>date_time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>humidity</th>\n",
       "      <th>sunshine</th>\n",
       "      <th>solar_radiation</th>\n",
       "      <th>power_consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_20220601 00</td>\n",
       "      <td>1</td>\n",
       "      <td>20220601 00</td>\n",
       "      <td>18.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1085.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_20220601 01</td>\n",
       "      <td>1</td>\n",
       "      <td>20220601 01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1047.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_20220601 02</td>\n",
       "      <td>1</td>\n",
       "      <td>20220601 02</td>\n",
       "      <td>17.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>974.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_20220601 03</td>\n",
       "      <td>1</td>\n",
       "      <td>20220601 03</td>\n",
       "      <td>16.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>953.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_20220601 04</td>\n",
       "      <td>1</td>\n",
       "      <td>20220601 04</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>986.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_date_time  building_number    date_time  temperature  rainfall  \\\n",
       "0  1_20220601 00                1  20220601 00         18.6       NaN   \n",
       "1  1_20220601 01                1  20220601 01         18.0       NaN   \n",
       "2  1_20220601 02                1  20220601 02         17.7       NaN   \n",
       "3  1_20220601 03                1  20220601 03         16.7       NaN   \n",
       "4  1_20220601 04                1  20220601 04         18.4       NaN   \n",
       "\n",
       "   windspeed  humidity  sunshine  solar_radiation  power_consumption  \n",
       "0        0.9      42.0       NaN              NaN            1085.28  \n",
       "1        1.1      45.0       NaN              NaN            1047.36  \n",
       "2        1.5      45.0       NaN              NaN             974.88  \n",
       "3        1.4      48.0       NaN              NaN             953.76  \n",
       "4        2.8      43.0       NaN              NaN             986.40  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lightgbm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "t = time.strftime('%m%d-%H%M', time.localtime(time.time()))\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정\n",
    "#데이터 불러오기\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "building_info =  pd.read_csv('./data/building_info.csv')\n",
    "\n",
    "train_df = train_df.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "test_df = test_df.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "building_info = building_info.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '건물유형': 'building_type',\n",
    "    '연면적(m2)': 'total_area',\n",
    "    '냉방면적(m2)': 'cooling_area',\n",
    "    '태양광용량(kW)': 'solar_power_capacity',\n",
    "    'ESS저장용량(kWh)': 'ess_capacity',\n",
    "    'PCS용량(kW)': 'pcs_capacity'\n",
    "})\n",
    "translation_dict = {\n",
    "    '건물기타': 'Other Buildings',\n",
    "    '공공': 'Public',\n",
    "    '대학교': 'University',\n",
    "    '데이터센터': 'Data Center',\n",
    "    '백화점및아울렛': 'Department Store and Outlet',\n",
    "    '병원': 'Hospital',\n",
    "    '상용': 'Commercial',\n",
    "    '아파트': 'Apartment',\n",
    "    '연구소': 'Research Institute',\n",
    "    '지식산업센터': 'Knowledge Industry Center',\n",
    "    '할인마트': 'Discount Mart',\n",
    "    '호텔및리조트': 'Hotel and Resort'\n",
    "}\n",
    "\n",
    "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "def merge(df):\n",
    "    df = df.fillna(0)\n",
    "    date = pd.to_datetime(df.date_time)\n",
    "    df['hour'] = date.dt.hour\n",
    "    df['day'] = date.dt.weekday\n",
    "    df['month'] = date.dt.month\n",
    "    df['week'] = date.dt.isocalendar().week\n",
    "    \n",
    "    # 'building_number'를 기준으로 두 데이터프레임 병합 및 전처리\n",
    "    merged_df = pd.merge(df, building_info, on='building_number',how='right')\n",
    "    #merge후 전처리\n",
    "    merged_df['solar_power_capacity'] = merged_df['solar_power_capacity'].replace('-', 0)\n",
    "    merged_df['ess_capacity'] = merged_df['ess_capacity'].replace('-', 0)\n",
    "    merged_df['pcs_capacity'] = merged_df['pcs_capacity'].replace('-', 0)\n",
    "\n",
    "    merged_df['solar_power_capacity'] = merged_df['solar_power_capacity'].astype('float64')\n",
    "    merged_df['ess_capacity'] = merged_df['ess_capacity'].astype('float64')\n",
    "    merged_df['pcs_capacity'] = merged_df['pcs_capacity'].astype('float64')\n",
    "    return merged_df\n",
    "\n",
    "merged_train_df0 = merge(train_df)\n",
    "merged_test_df0 = merge(test_df)\n",
    "\n",
    "\n",
    "kk = merged_train_df0.loc[(merged_train_df0.building_number == 95)&(merged_train_df0.hour == 16)&(merged_train_df0.day == 2), 'power_consumption']\n",
    "sum16 = kk.sum()/12\n",
    "kkk = merged_train_df0.loc[(merged_train_df0.building_number == 95)&(merged_train_df0.hour == 17)&(merged_train_df0.day == 2), 'power_consumption']\n",
    "sum17 = (kkk.sum()-0.36)/12\n",
    "merged_train_df0.loc[(merged_train_df0.building_number == 95)&(merged_train_df0.hour == 16)&(merged_train_df0.day == 2)&(merged_train_df0.month == 7)&(merged_train_df0.week == 30), 'power_consumption'] = sum16\n",
    "merged_train_df0.loc[(merged_train_df0.building_number == 95)&(merged_train_df0.hour == 17)&(merged_train_df0.day == 2)&(merged_train_df0.month == 7)&(merged_train_df0.week == 30), 'power_consumption'] = sum17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df['holiday'] = df.apply(lambda x: 0 if x['day'] < 5 else 1, axis=1)\n",
    "    df.loc[((df['month'] == 6) & (df['day'] == 2) &(df['week'] == 22), 'holiday')] = 1\n",
    "    df.loc[((df['month'] == 6) & (df['day'] == 0) &(df['week'] == 23), 'holiday')] = 1\n",
    "    df.loc[((df['month'] == 8) & (df['day'] == 0) &(df['week'] == 33), 'holiday')] = 1\n",
    "    #######################################\n",
    "    ## 건물별, 요일별, 전력시간별 소비량 평균 넣어주기\n",
    "    #######################################\n",
    "    power_mean = pd.pivot_table(merged_train_df0, values = 'power_consumption', index = ['building_number', 'hour', 'day'], aggfunc = np.mean).reset_index()\n",
    "    #######################################\n",
    "    ## 건물별, 요일별, 전력시간별 소비량 표준편차 넣어주기\n",
    "    #######################################\n",
    "    power_std = pd.pivot_table(merged_train_df0, values = 'power_consumption', index = ['building_number', 'hour', 'day'], aggfunc = np.std).reset_index()\n",
    "    #######################################\n",
    "    ## 건물별 시간별 전력소비량 평균 넣어주기\n",
    "    #######################################\n",
    "    power_hour_mean = pd.pivot_table(merged_train_df0, values = 'power_consumption', index = ['building_number', 'hour'], aggfunc = np.mean).reset_index()\n",
    "    #######################################\n",
    "    ## 건물별 시간별 전력소비량 표준편차 넣어주기\n",
    "    #######################################\n",
    "    power_hour_std = pd.pivot_table(merged_train_df0, values = 'power_consumption', index = ['building_number', 'hour'], aggfunc = np.std).reset_index()\n",
    "    \n",
    "    # 병합을 위한 키 설정\n",
    "    merge_keys = ['building_number', 'hour', 'day']  \n",
    "    # 데이터프레임 병합\n",
    "    df = df.merge(power_mean[merge_keys + ['power_consumption']], on=merge_keys, how='left', suffixes=('', '_day_hour_mean'))\n",
    "    df = df.merge(power_std[merge_keys + ['power_consumption']], on=merge_keys, how='left', suffixes=('', '_day_hour_std'))\n",
    "    df = df.merge(power_hour_mean[merge_keys[:-1] + ['power_consumption']], on=merge_keys[:-1], how='left', suffixes=('', '_hour_mean'))\n",
    "    df = df.merge(power_hour_std[merge_keys[:-1] + ['power_consumption']], on=merge_keys[:-1], how='left', suffixes=('', '_hour_std'))\n",
    "    df = df.rename(columns = {'power_consumption_day_hour_mean':'day_hour_mean','power_consumption_day_hour_std':'day_hour_std','power_consumption_hour_mean':'hour_mean','power_consumption_hour_std':'hour_std',})\n",
    "    \n",
    "    def CDH(xs):\n",
    "        ys = []\n",
    "        for i in range(len(xs)):\n",
    "            if i < 11:\n",
    "                ys.append(np.sum(xs[:(i+1)]-26))\n",
    "            else:\n",
    "                ys.append(np.sum(xs[(i-11):(i+1)]-26))\n",
    "        return np.array(ys)\n",
    "\n",
    "    cdhs = np.array([])\n",
    "    for num in range(1,101,1):\n",
    "        temp = df[df['building_number'] == num]\n",
    "        cdh = CDH(temp['temperature'].values)\n",
    "        cdhs = np.concatenate([cdhs, cdh])\n",
    "    df['CDH'] = cdhs\n",
    "     \n",
    "    ## https://dacon.io/competitions/official/235680/codeshare/2366?page=1&dtype=recent\n",
    "    df['sin_time'] = np.sin(2*np.pi*df.hour/24)\n",
    "    df['cos_time'] = np.cos(2*np.pi*df.hour/24)\n",
    "    \n",
    "    ## https://dacon.io/competitions/official/235736/codeshare/2743?page=1&dtype=recent\n",
    "    df['THI'] = 9/5*df['temperature'] - 0.55*(1-df['humidity']/100)*(9/5*df['humidity']-26)+32\n",
    "    \n",
    "    return df\n",
    "\n",
    "merged_train_df1 = preprocessing(merged_train_df0)\n",
    "merged_test_df1 = preprocessing(merged_test_df0).rename(columns = {'power_consumption':'day_hour_mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "vaild = pd.DataFrame()\n",
    "for i in range(1,101,1):\n",
    "    buff = merged_train_df1.loc[(merged_train_df1.building_number == i)] \n",
    "    train = pd.concat([train, buff[:-168]])\n",
    "    vaild = pd.concat([vaild, buff[-168:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_train(df):\n",
    "    \n",
    "    grouped = df.groupby(['building_number', 'date_time'])\n",
    "    df['max_power'] = grouped['power_consumption'].transform(np.max)\n",
    "    df['min_power'] = grouped['power_consumption'].transform(np.min)\n",
    "    \n",
    "    grouped2 = df.groupby(['building_number', 'day'])\n",
    "    df['max_power'] = grouped2['max_power'].transform(np.mean)\n",
    "    df['min_power'] = grouped2['min_power'].transform(np.mean)\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    \n",
    "    return df.drop(columns=['num_date_time','date_time','building_type','sunshine','pcs_capacity','ess_capacity','solar_power_capacity','solar_radiation','total_area','cooling_area'])\n",
    "\n",
    "def data_test(df):\n",
    "    grouped_train = merged_train_df.groupby(['building_number', 'day'])\n",
    "    max_power = grouped_train['max_power'].median()\n",
    "    min_power = grouped_train['min_power'].median()\n",
    "    max_power_df = max_power.reset_index()\n",
    "    min_power_df = min_power.reset_index()\n",
    "\n",
    "    # 테스트 데이터에 max_power_median와 min_power_median을 추가\n",
    "    df = pd.merge(df, max_power_df, on=['building_number', 'day'])\n",
    "    df = pd.merge(df, min_power_df, on=['building_number', 'day'])\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "\n",
    "    return df.drop(columns=['num_date_time','date_time','building_type','pcs_capacity','ess_capacity','solar_power_capacity','total_area','cooling_area'])\n",
    "\n",
    "merged_train_df =data_train(merged_train_df1)\n",
    "merged_test_df =data_test(merged_test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_train(train)\n",
    "vaild = data_test(vaild).drop(columns=['sunshine','solar_radiation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, test 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the preprocessed data\n",
    "merged_train_df.to_csv(f'./split/train_preprocessed_wsw_{t}.csv')\n",
    "merged_test_df.to_csv(f'./split/test_preprocessed_wsw_{t}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vaild저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the preprocessed data\n",
    "train.to_csv(f'./split/train_split_preprocessed_wsw_{t}.csv')\n",
    "vaild.to_csv(f'./split/vaild_preprocessed_wsw_{t}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빌딩별로 csv생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train,test\n",
    "train_df  = pd.read_csv((f'./split/train_preprocessed_wsw_0803-0021.csv')).drop(columns='Unnamed: 0')\n",
    "test_df  = pd.read_csv((f'./split/vaild_preprocessed_wsw_0803-0021.csv')).drop(columns='Unnamed: 0')\n",
    "for i in range(1,101,1):\n",
    "    tr = train_df.loc[train_df.building_number==i].reset_index()\n",
    "    tr = tr.drop(columns=['index'])\n",
    "    te = test_df.loc[test_df.building_number==i].reset_index()\n",
    "    te = te.drop(columns=['index'])\n",
    "    tr.to_csv(f'./split/train_building{i}.csv')\n",
    "    te.to_csv(f'./split/test_building{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['index'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m101\u001b[39m,\u001b[39m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     trv \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mloc[train\u001b[39m.\u001b[39mbuilding_number\u001b[39m==\u001b[39mi]\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m----> 6\u001b[0m     trv \u001b[39m=\u001b[39m tr\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      7\u001b[0m     v \u001b[39m=\u001b[39m vaild\u001b[39m.\u001b[39mloc[vaild\u001b[39m.\u001b[39mbuilding_number\u001b[39m==\u001b[39mi]\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m      8\u001b[0m     v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5259\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5260\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5261\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5262\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5263\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5264\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5265\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5266\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['index'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#train,valid\n",
    "train  = pd.read_csv((f'./split/train_split_preprocessed_wsw_0803-0021.csv')).drop(columns='Unnamed: 0')\n",
    "vaild  = pd.read_csv((f'./split/vaild_preprocessed_wsw_0803-0021.csv')).drop(columns='Unnamed: 0')\n",
    "for i in range(1,101,1):\n",
    "    trv = train.loc[train.building_number==i].reset_index()\n",
    "    trv = trv.drop(columns=['index'])\n",
    "    v = vaild.loc[vaild.building_number==i].reset_index()\n",
    "    v = v.drop(columns=['index'])\n",
    "    trv.to_csv(f'./split/train_valid_building{i}.csv')\n",
    "    v.to_csv(f'./split/valid_building{i}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
