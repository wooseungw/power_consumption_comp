{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1872, 19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./pretest/train_building37.csv').drop(columns=[ 'week', 'THI', 'CDH', 'max_power', 'min_power' ])\n",
    "valid = pd.read_csv('./pretest/pretest_building37.csv')\n",
    "train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 76, 42, 18])\n",
      "torch.Size([24, 76, 42, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def sliding_window(tensor, window_size, step_size=1, batch_size=24):\n",
    "    windows = []\n",
    "    for i in range(0, tensor.size(0) - window_size + 1, step_size):\n",
    "        windows.append(tensor[i:i+window_size])\n",
    "    windows = torch.stack(windows)\n",
    "    \n",
    "    # 배치로 나누기\n",
    "    num_batches = windows.size(0) // batch_size\n",
    "    windows = windows[:num_batches * batch_size]\n",
    "    windows_batched = windows.view(batch_size, -1, *windows.shape[1:])\n",
    "    \n",
    "    return windows_batched\n",
    "\n",
    "# 예제 데이터\n",
    "data_18 = torch.randn(1872, 18)\n",
    "data_1 = torch.randn(1872, 1)\n",
    "\n",
    "window_size = 42\n",
    "\n",
    "batched_data_18 = sliding_window(data_18, window_size)\n",
    "batched_data_1 = sliding_window(data_1, window_size)\n",
    "\n",
    "print(batched_data_18.shape)  # [24, 42, 18]\n",
    "print(batched_data_1.shape)   # [24, 42, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 Tensor 형태의 데이터를 모델에 전달합니다.\n",
    "x_train = train.drop(columns=['power_consumption'])\n",
    "y_train = train['power_consumption']\n",
    "\n",
    "\n",
    "#312\n",
    "x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values[:, None], dtype=torch.float32)\n",
    "batch = 24\n",
    "sequence_length = 48\n",
    "feature_embed_dim = 18\n",
    "\n",
    "# class Transformers(nn.Module):\n",
    "#     def __init__(self, input_dim,d_model,nhead,encoder_layers):\n",
    "#         super(Transformers,self).__init__()\n",
    "        \n",
    "#     def forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m x_train_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(x_train\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m# (sequence_length, batch_size, feature_dim)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m y_train_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_train\u001b[39m.\u001b[39mvalues[:, \u001b[39mNone\u001b[39;00m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msqueeze() \u001b[39m# (sequence_length, batch_size)\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m output \u001b[39m=\u001b[39m model(x_train_tensor, y_train_tensor)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Positional Encoding\u001b[39;00m\n\u001b[0;32m     26\u001b[0m src_positions \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, src\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(src\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), src\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)))\u001b[39m.\u001b[39mto(src\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 27\u001b[0m tgt_positions \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, tgt\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(tgt\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), tgt\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m)))\u001b[39m.\u001b[39mto(tgt\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     28\u001b[0m src \u001b[39m=\u001b[39m src \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(src_positions)\n\u001b[0;32m     29\u001b[0m tgt \u001b[39m=\u001b[39m tgt \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(tgt_positions)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Embedding Layer: 시계열 데이터를 d_model 차원으로 임베딩합니다.\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional Encoding: 시퀀스의 위치 정보를 제공합니다.\n",
    "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
    "        \n",
    "        # Transformer Layer: Encoder와 Decoder로 구성됩니다.\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        \n",
    "        # Output Layer: 예측값을 출력합니다.\n",
    "        self.out = nn.Linear(d_model, 168)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # src와 tgt는 각각 (S, N, E)와 (T, N, E)의 shape을 가집니다.\n",
    "        # S는 source sequence length, T는 target sequence length, N은 batch size, E는 feature number입니다.\n",
    "        \n",
    "        # Embedding\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.embedding.out_features, dtype=torch.float32))\n",
    "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.embedding.out_features, dtype=torch.float32))\n",
    "        \n",
    "        # Positional Encoding\n",
    "        src_positions = (torch.arange(0, src.size(0)).unsqueeze(1).expand(src.size(0), src.size(1))).to(src.device)\n",
    "        tgt_positions = (torch.arange(0, tgt.size(0)).unsqueeze(1).expand(tgt.size(0), tgt.size(1))).to(tgt.device)\n",
    "        src = src + self.pos_encoder(src_positions)\n",
    "        tgt = tgt + self.pos_encoder(tgt_positions)\n",
    "        \n",
    "        # Transformer\n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask, None, None, src_key_padding_mask, tgt_key_padding_mask)\n",
    "        \n",
    "        # Output Layer\n",
    "        return self.out(output)\n",
    "\n",
    "# 예제\n",
    "input_dim = 1872 # 예를 들어, 전력소비량만을 입력으로 사용한다면 1\n",
    "d_model = 18\n",
    "nhead = 6\n",
    "\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 2040\n",
    "#모델선언\n",
    "model = TransformerModel(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = model(x_train_tensor, y_train_tensor)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
