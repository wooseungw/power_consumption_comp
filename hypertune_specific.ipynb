{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightgbm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, TimeSeriesSplit\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sktime.utils.plotting import plot_series\n",
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "t = time.strftime('%m%d-%H%M', time.localtime(time.time()))\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정\n",
    "#### alpha를 argument로 받는 함수로 실제 objective function을 wrapping하여 alpha값을 쉽게 조정할 수 있도록 작성했습니다.\n",
    "# custom objective function for forcing model not to underestimate\n",
    "def weighted_mse(alpha = 1):\n",
    "    def weighted_mse_fixed(label, pred):\n",
    "        residual = (label - pred).astype(\"float\")\n",
    "        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n",
    "        hess = np.where(residual>0, 2*alpha, 2.0)\n",
    "        return grad, hess\n",
    "    return weighted_mse_fixed\n",
    "\n",
    " #점수 측정을 위한 코드\n",
    "def SMAPE(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "building_1\n",
      "|   iter    |  target   |    eta    | n_esti... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-5.027   \u001b[0m | \u001b[0m0.004371 \u001b[0m | \u001b[0m9.51e+03 \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-5.128   \u001b[0m | \u001b[0m0.007588 \u001b[0m | \u001b[0m6.007e+03\u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-5.448   \u001b[0m | \u001b[0m0.002404 \u001b[0m | \u001b[0m1.602e+03\u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-5.07    \u001b[0m | \u001b[0m0.001523 \u001b[0m | \u001b[0m8.668e+03\u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-5.108   \u001b[0m | \u001b[0m0.00641  \u001b[0m | \u001b[0m7.095e+03\u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-5.085   \u001b[0m | \u001b[0m0.001185 \u001b[0m | \u001b[0m9.701e+03\u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-5.062   \u001b[0m | \u001b[0m0.008492 \u001b[0m | \u001b[0m2.163e+03\u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-5.132   \u001b[0m | \u001b[0m0.006163 \u001b[0m | \u001b[0m9.146e+03\u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-5.146   \u001b[0m | \u001b[0m0.005031 \u001b[0m | \u001b[0m9.508e+03\u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-5.066   \u001b[0m | \u001b[0m0.001912 \u001b[0m | \u001b[0m5.241e+03\u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-5.185   \u001b[0m | \u001b[0m0.001051 \u001b[0m | \u001b[0m4.665e+03\u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-5.137   \u001b[0m | \u001b[0m0.007008 \u001b[0m | \u001b[0m6.44e+03 \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = pd.DataFrame(columns = ['n_estimators', 'eta', 'min_child_weight','max_depth', 'colsample_bytree', 'subsample'])\n",
    "\n",
    "idnex = [1,2,7,10,11,12,13,14,15,26,27,28,29,30,31,32,33,34,35,56,59,61,62,63,64,66,67,68,69,72,73,83,87,88,89,91,92,94,99,100]\n",
    "\n",
    "print(len(idnex))\n",
    "for i in [1]:\n",
    "    print(f'building_{i}')\n",
    "    train_df = pd.read_csv(f'./submit/train_building{i}.csv')\n",
    "    \n",
    "    t_y = train_df['power_consumption']\n",
    "    t_x = train_df.drop(['power_consumption'], axis=1)\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = t_y, X = t_x, test_size = 168)\n",
    "    \n",
    "        # 목적 함수 정의\n",
    "    def xgb_evaluate(n_estimators, eta):\n",
    "        params = {\n",
    "            'n_estimators': int(n_estimators),\n",
    "            'eta': eta,\n",
    "            'objective': 'reg:squarederror',\n",
    "            \n",
    "        }\n",
    "        model = XGBRegressor(**params,tree_method=\"hist\", gpu_id=0)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_valid)\n",
    "        return -SMAPE(y_valid, y_pred)\n",
    "\n",
    "    # 베이지안 최적화 객체 생성\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=xgb_evaluate,\n",
    "        pbounds={\n",
    "            'n_estimators': (50, 10000),\n",
    "            'eta': (0.001, 0.01)\n",
    "        },\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 최적화 실행\n",
    "    \n",
    "    optimizer.maximize(init_points=7, n_iter=5)\n",
    "    best_params  = optimizer.max['params']\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "    hyperparameters.loc[i-1] = best_params\n",
    "    hyperparameters.to_csv(f'./parameters/hyperparameter_xgb_wsw{t}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________Buildding Number:1 Hyperparameter Tuning__________\n",
      "Fitting 1 folds for each of 240 candidates, totalling 240 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 31\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m#y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#fit\u001b[39;00m\n\u001b[0;32m     26\u001b[0m gcv \u001b[39m=\u001b[39m GridSearchCV(estimator \u001b[39m=\u001b[39m XGBRegressor(tree_method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhist\u001b[39m\u001b[39m\"\u001b[39m,  gpu_id\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, seed \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m), \n\u001b[0;32m     27\u001b[0m                    param_grid\u001b[39m=\u001b[39mgrid, \n\u001b[0;32m     28\u001b[0m                    scoring\u001b[39m=\u001b[39msmape_score, \n\u001b[0;32m     29\u001b[0m                    cv\u001b[39m=\u001b[39mPredefinedSplit(np\u001b[39m.\u001b[39mappend(\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(x)\u001b[39m-\u001b[39m\u001b[39m168\u001b[39m), np\u001b[39m.\u001b[39mzeros(\u001b[39m168\u001b[39m))),\n\u001b[0;32m     30\u001b[0m                    refit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m gcv\u001b[39m.\u001b[39;49mfit(x, y)\n\u001b[0;32m     33\u001b[0m best \u001b[39m=\u001b[39m gcv\u001b[39m.\u001b[39mbest_estimator_\n\u001b[0;32m     34\u001b[0m params \u001b[39m=\u001b[39m gcv\u001b[39m.\u001b[39mbest_params_ \n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\joblib\\parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1854\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1855\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1857\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1784\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1785\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1786\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\xgboost\\sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m (\n\u001b[0;32m   1017\u001b[0m     model,\n\u001b[0;32m   1018\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1024\u001b[0m )\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1026\u001b[0m     params,\n\u001b[0;32m   1027\u001b[0m     train_dmatrix,\n\u001b[0;32m   1028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1029\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1030\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1031\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1032\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1033\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1034\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1035\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1036\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1037\u001b[0m )\n\u001b[0;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1040\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\poum\\Lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid = {'n_estimators': [hyperparameters.at[i-1,'n_estimators']], \n",
    "        'eta':[hyperparameters.at[i-1,'eta']],'min_child_weight' : np.arange(1, 11, 1),\n",
    "        'max_depth' : np.arange(2,8,1) , 'colsample_bytree' :[0.8, 0.9],\n",
    "        'subsample' : [0.8, 0.9]}\n",
    "\n",
    "#tscv = TimeSeriesSplit(n_splits=3)\n",
    "smape_score = make_scorer(SMAPE, greater_is_better=False)\n",
    "\n",
    "for i in [1]:\n",
    "    train_df = pd.read_csv(f'./submit/train_building{i}.csv')\n",
    "    # pretest_df = pd.read_csv(f'./pretest/pretest_building{i}.csv')\n",
    "    ################################################\n",
    "    #merge = pd.concat([train_df,pretest_df])\n",
    "    ################################################\n",
    "    #하이퍼 파라미터 튜닝 method 1\n",
    "    #leaf method\n",
    "    print(f\"__________Buildding Number:{i} Hyperparameter Tuning__________\")\n",
    "\n",
    "    # preds = np.array([])\n",
    "    grid_under = {}\n",
    "    #합병된 데이터 사용\n",
    "    y = train_df['power_consumption']\n",
    "    x = train_df.drop(columns=['power_consumption'])\n",
    "    #y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    #fit\n",
    "    gcv = GridSearchCV(estimator = XGBRegressor(tree_method=\"hist\",  gpu_id=0, seed = 0), \n",
    "                       param_grid=grid, \n",
    "                       scoring=smape_score, \n",
    "                       cv=PredefinedSplit(np.append(-np.ones(len(x)-168), np.zeros(168))),\n",
    "                       refit=True, verbose=True)\n",
    "    gcv.fit(x, y)\n",
    "\n",
    "    best = gcv.best_estimator_\n",
    "    params = gcv.best_params_ \n",
    "    hyperparameters.loc[i-1] = best_params\n",
    "    #pred\n",
    "    # x_pretest = pretest_df.drop(columns=['power_consumption'])\n",
    "    # y_pretest = pretest_df['power_consumption']\n",
    "    # pred = best.predict(x_pretest)\n",
    "    print(\"_____Best Parameters_____\",params)\n",
    "    # print(\"_____SMAPE Score________\", SMAPE(y_pretest, pred))\n",
    "    \n",
    "hyperparameters.to_csv(f'./specific/hyperparameter_xgb_wsw{t}.csv', index=False) # save the tuned parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = pd.read_csv(f'./parameters/hyperparameter_xgb_wsw{t}.csv')\n",
    "#alpha\n",
    "alpha_list = []\n",
    "smape_list = []\n",
    "for i in tqdm(range(100)):\n",
    "    train_df = pd.read_csv(f'./pretest/train_building{i+1}.csv')\n",
    "    pretest_df = pd.read_csv(f'./pretest/pretest_building{i+1}.csv')\n",
    "    \n",
    "    ################################################\n",
    "    t_y = train_df['power_consumption']\n",
    "    t_x = train_df.drop(['power_consumption'], axis=1)\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = t_y, X = t_x, test_size = 168)\n",
    "    \n",
    "    \n",
    "    p_y = pretest_df['power_consumption']\n",
    "    p_x = pretest_df.drop(['power_consumption'], axis=1)\n",
    "\n",
    "    xgb = XGBRegressor(seed = 0, tree_method=\"hist\",  gpu_id=0,\n",
    "                    n_estimators = hyperparameters.at[i-1,'n_estimators'], \n",
    "                    eta = hyperparameters.at[i-1,'eta'],\n",
    "                    min_child_weight = hyperparameters.at[i-1,'min_child_weight'],\n",
    "                    max_depth = hyperparameters.at[i-1,'max_depth'], \n",
    "                    colsample_bytree = hyperparameters.at[i-1,'colsample_bytree'], \n",
    "                    subsample = hyperparameters.at[i-1,'subsample'])\n",
    "\n",
    "    xgb.fit(x_train, y_train ,verbose=False)\n",
    "    pred0 = xgb.predict(x_valid)\n",
    "    best_alpha = 0\n",
    "    score0 = SMAPE(y_valid,pred0)\n",
    "\n",
    "    for j in [1, 3, 5, 7, 10, 25, 50, 75, 100]:\n",
    "        xgb = XGBRegressor(seed = 0, tree_method=\"hist\",  gpu_id=0,\n",
    "                    n_estimators = hyperparameters.at[i-1,'n_estimators'], \n",
    "                    eta = hyperparameters.at[i-1,'eta'],\n",
    "                    min_child_weight = hyperparameters.at[i-1,'min_child_weight'],\n",
    "                    max_depth = hyperparameters.at[i-1,'max_depth'], \n",
    "                    colsample_bytree = hyperparameters.at[i-1,'colsample_bytree'], \n",
    "                    subsample = hyperparameters.at[i-1,'subsample'])\n",
    "        xgb.set_params(**{'objective' : weighted_mse(j)})\n",
    "\n",
    "        xgb.fit(x_train, y_train,verbose=False)\n",
    "        \n",
    "        pred1 = xgb.predict(x_valid)\n",
    "        score1 = SMAPE(y_valid, pred1)\n",
    "        if score1 < score0: \n",
    "            best_alpha = j\n",
    "            score0 = score1\n",
    "\n",
    "    alpha_list.append(best_alpha)\n",
    "    smape_list.append(score0)\n",
    "    \n",
    "    print(\"building {} || best score : {} || alpha : {}\".format(i+1, score0, best_alpha))\n",
    "    \n",
    "    \n",
    "hyperparameters['alpha'] = alpha_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
